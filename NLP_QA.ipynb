{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KFSgtHbyvrE"
   },
   "source": [
    "### 1. Data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnEQEsXpBaMd",
    "outputId": "63112da4-93cc-4a4c-ed0e-41941afc14dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-05 06:13:59--  https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4118001 (3.9M) [application/zip]\n",
      "Saving to: ‘BoolQ.zip’\n",
      "\n",
      "BoolQ.zip           100%[===================>]   3.93M  8.09MB/s    in 0.5s    \n",
      "\n",
      "2021-09-05 06:14:00 (8.09 MB/s) - ‘BoolQ.zip’ saved [4118001/4118001]\n",
      "\n",
      "Archive:  BoolQ.zip\n",
      "   creating: BoolQ/\n",
      "  inflating: BoolQ/train.jsonl       \n",
      "  inflating: BoolQ/test.jsonl        \n",
      "  inflating: BoolQ/val.jsonl         \n"
     ]
    }
   ],
   "source": [
    "#download the data\n",
    "!wget https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip\n",
    "!unzip BoolQ.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CWrrAA4bJQON"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5BwkFRfRuDH",
    "outputId": "f2663c35-4e93-48e7-eaf1-f7dfe8988d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "33luNlvCB2U9"
   },
   "outputs": [],
   "source": [
    "# Load training and validation datasets into a pandas dataframe\n",
    "train_df = pd.read_json(\"./BoolQ/train.jsonl\", orient='records', lines=True)\n",
    "val_df = pd.read_json(\"./BoolQ/val.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "tB5pZVE_z_mt",
    "outputId": "a1a15352-c0c7-4d65-c5b9-bf7d050d2cb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>passage</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>is it illegal to flash lights to warn of police</td>\n",
       "      <td>Headlight flashing -- In the United States, al...</td>\n",
       "      <td>2191</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>behind enemy lines was it based on a true story</td>\n",
       "      <td>Scott O'Grady -- Scott Francis O'Grady (born O...</td>\n",
       "      <td>968</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>is a star is born with lady gaga a remake</td>\n",
       "      <td>A Star Is Born (2018 film) -- A Star Is Born i...</td>\n",
       "      <td>891</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5616</th>\n",
       "      <td>has anyone ever won the moment of truth</td>\n",
       "      <td>The Moment of Truth (U.S. game show) -- Howeve...</td>\n",
       "      <td>5616</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7924</th>\n",
       "      <td>does the bride have to wear a white dress</td>\n",
       "      <td>Wedding dress -- A wedding dress or wedding go...</td>\n",
       "      <td>7924</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  ...  label\n",
       "2191  is it illegal to flash lights to warn of police  ...  False\n",
       "968   behind enemy lines was it based on a true story  ...   True\n",
       "891         is a star is born with lady gaga a remake  ...   True\n",
       "5616          has anyone ever won the moment of truth  ...   True\n",
       "7924        does the bride have to wear a white dress  ...  False\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "jhEhnATeQg_a",
    "outputId": "c2d9d138-35cf-4ff5-a4ac-51c53a469593"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>passage</th>\n",
       "      <th>idx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>do playstation 2 games play on playstation 4</td>\n",
       "      <td>List of PlayStation 2 games for PlayStation 4 ...</td>\n",
       "      <td>1295</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>does the us constitution allow states to secede</td>\n",
       "      <td>Secession in the United States -- The Constitu...</td>\n",
       "      <td>2883</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>can you play gameboy games on a ds lite</td>\n",
       "      <td>Nintendo DS Lite -- The Nintendo DS Lite is co...</td>\n",
       "      <td>2368</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>is the tower of terror in magic kingdom</td>\n",
       "      <td>The Twilight Zone Tower of Terror -- The Twili...</td>\n",
       "      <td>2474</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>has brazil ever won the world cup in europe</td>\n",
       "      <td>Brazil at the FIFA World Cup -- Brazil is the ...</td>\n",
       "      <td>1412</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  ...  label\n",
       "1295     do playstation 2 games play on playstation 4  ...   True\n",
       "2883  does the us constitution allow states to secede  ...  False\n",
       "2368          can you play gameboy games on a ds lite  ...   True\n",
       "2474          is the tower of terror in magic kingdom  ...  False\n",
       "1412      has brazil ever won the world cup in europe  ...   True\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Tp3lBUw0cnO"
   },
   "source": [
    "Basic statistics (example number, class distribution, mean sentence length, number of unique words, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-7hWcQfBuaG",
    "outputId": "83fccfe8-7e22-4747-c734-08509ce719e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 9,427\n",
      "\n",
      "Number of validation data: 3,270\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#number of sentences\n",
    "print('Number of training data: {:,}\\n'.format(train_df.shape[0]))\n",
    "print('Number of validation data: {:,}\\n'.format(val_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyaiBKX90PKx",
    "outputId": "188d9b77-ba89-4aad-ed79-2d920feda11c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution of training data:\n",
      " {True: 0.6231038506417736, False: 0.37689614935822635}\n",
      "Class distribution of validation data:\n",
      " {True: 0.6217125382262997, False: 0.3782874617737003}\n"
     ]
    }
   ],
   "source": [
    "print('Class distribution of training data:\\n', (train_df.label.value_counts() / train_df.shape[0]).to_dict())\n",
    "print('Class distribution of validation data:\\n', (val_df.label.value_counts() / val_df.shape[0]).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s7AthJ17jES"
   },
   "source": [
    "So, the majority-class baseline accuracy for validation data is 62.2% (only 'yes' answers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "VLbMfwJxrv4s",
    "outputId": "7b5917af-8605-497c-885f-b937e3b163bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Val')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 3))\n",
    "fig.suptitle('Class distribution')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_df.label.value_counts().plot(kind='bar')\n",
    "plt.grid()\n",
    "plt.title('Train')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "val_df.label.value_counts().plot(kind='bar')\n",
    "plt.grid()\n",
    "plt.title('Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rI27Mejp2Mj-",
    "outputId": "b254b929-0eab-4471-de25-669ecbcec05e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sentence length (in words) of training data: 106.26307414872176\n",
      "Mean sentence length (in words) of validation data: 105.11192660550459\n"
     ]
    }
   ],
   "source": [
    "print('Mean sentence length (in words) of training data:', (train_df.question + ' ' + train_df.passage).apply(lambda x: len(x.split())).mean())\n",
    "print('Mean sentence length (in words) of validation data:', (val_df.question + ' ' + val_df.passage).apply(lambda x: len(x.split())).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hFH7x5T3F1g",
    "outputId": "caa32819-0363-4cbd-85d9-3fd4d6a09773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sentence length (in symbols) of training data: 635.1599660549485\n",
      "Mean sentence length (in symbols) of validation data: 627.5125382262997\n"
     ]
    }
   ],
   "source": [
    "print('Mean sentence length (in symbols) of training data:', (train_df.question + ' ' + train_df.passage).apply(lambda x: len(x)).mean())\n",
    "print('Mean sentence length (in symbols) of validation data:', (val_df.question + ' ' + val_df.passage).apply(lambda x: len(x)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b6nB6Faw4YH5"
   },
   "outputs": [],
   "source": [
    "#tokenize function\n",
    "def letters_digits_only(text):\n",
    "    prepare_text = re.sub('[^a-z0-9 ]', ' ', text.lower())\n",
    "    prepare_text = re.sub('[ ]{2,}', ' ', prepare_text).strip()\n",
    "    tokens = [token for token in prepare_text.split()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBz9GkS-23ks",
    "outputId": "a67086a4-a3b2-4b8f-c3ac-dee31fb87a84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [do, iran, and, afghanistan, speak, the, same,...\n",
       "1    [do, good, samaritan, laws, protect, those, wh...\n",
       "2    [is, windows, movie, maker, part, of, windows,...\n",
       "3    [is, confectionary, sugar, the, same, as, powd...\n",
       "4    [is, elder, scrolls, online, the, same, as, sk...\n",
       "Name: qp_tokenized, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate questions and passages and tokenize them\n",
    "train_df['qp_tokenized'] = (train_df.question + ' ' + train_df.passage).map(letters_digits_only)\n",
    "val_df['qp_tokenized'] = (val_df.question + ' ' + val_df.passage).map(letters_digits_only)\n",
    "\n",
    "train_df['qp_tokenized'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHyU4q3G3Cma",
    "outputId": "9b2f8346-9654-4023-e23a-e770cee6be47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in training data: 42022\n",
      "Number of unique words in validation data: 25004\n"
     ]
    }
   ],
   "source": [
    "#unique words\n",
    "train_unique_words = set()\n",
    "val_unique_words = set()\n",
    "\n",
    "for s in train_df.qp_tokenized:\n",
    "    train_unique_words |= set(s)\n",
    "\n",
    "for s in val_df.qp_tokenized:\n",
    "    val_unique_words |= set(s)\n",
    "\n",
    "print('Number of unique words in training data:', len(train_unique_words))\n",
    "print('Number of unique words in validation data:', len(val_unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq2iveC2whUG"
   },
   "source": [
    "### 2. Pretrained embeddings as features for classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwm-lT7MwswC"
   },
   "source": [
    "* 2.1. Data vectorizing with pre-trained word2vec embeddings (GoogleNews vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EklClJpvx_U",
    "outputId": "5382e29d-1a34-4377-dfc7-a6c391b20fa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-05 06:15:04--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.87.238\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.87.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1647046227 (1.5G) [application/x-gzip]\n",
      "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>]   1.53G  46.7MB/s    in 37s     \n",
      "\n",
      "2021-09-05 06:15:41 (42.7 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#download GoogleNews\n",
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "!gzip -d GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jUAt0Ak3yNUv"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "word2vec = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JkJWVWaiinFp"
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean([self.word2vec[word] for word in words if word in self.word2vec]\n",
    "                                 or [np.zeros(self.dim)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "2fe3m6dgyUj5"
   },
   "outputs": [],
   "source": [
    "vectorizer = MeanEmbeddingVectorizer(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QC_aXdbCzCsd"
   },
   "outputs": [],
   "source": [
    "#vectorize the data\n",
    "train_df_vec = vectorizer.transform(train_df.qp_tokenized.values)\n",
    "val_df_vec = vectorizer.transform(val_df.qp_tokenized.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT1tYCMb5tVm"
   },
   "source": [
    "* 2.2. Use pretrained embeddings as features to a classifier. Train SVM, Linear Regression or any other classification model. Describe and analyze your results (use accuracy metric and data from val.jsonl for evaluation of your results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W4JhpTQE6IAy"
   },
   "outputs": [],
   "source": [
    "answers_train = train_df.label.values.astype(int)\n",
    "answers_val = val_df.label.values.astype(int)\n",
    "\n",
    "questions_train = train_df.question.values\n",
    "passages_train = train_df.passage.values\n",
    "\n",
    "questions_val = val_df.question.values\n",
    "passages_val = val_df.passage.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "m5GN34i7UjBd"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cB3iJJGO5kwx",
    "outputId": "0856947d-d3da-4554-9984-74902ca872af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.6406727828746177\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "clf = SVC()\n",
    "clf.fit(train_df_vec, answers_train)\n",
    "answers_val_pred = clf.predict(val_df_vec)\n",
    "score = accuracy_score(answers_val_pred, answers_val)\n",
    "print('SVM accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "iWIQOoNn6voK"
   },
   "outputs": [],
   "source": [
    "#Data scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_train_df_vec = scaler.fit_transform(train_df_vec)\n",
    "scaled_val_df_vec = scaler.fit_transform(val_df_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WceZ_GtZ-vBj",
    "outputId": "bf25bbfe-93f4-46f5-b08b-2e63c5404a8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy with scaling: 0.6611620795107034\n"
     ]
    }
   ],
   "source": [
    "#SVM on scaled data\n",
    "clf = SVC()\n",
    "clf.fit(scaled_train_df_vec, answers_train)\n",
    "answers_val_pred = clf.predict(scaled_val_df_vec)\n",
    "score = accuracy_score(answers_val_pred, answers_val)\n",
    "print('SVM accuracy with scaling:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRCnMs8rty7H",
    "outputId": "bf3c3331-e3e0-453c-ef1a-353c1be8c236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.6321100917431193\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_df_vec, answers_train)\n",
    "answers_val_pred = clf.predict(val_df_vec)\n",
    "score = accuracy_score(answers_val_pred, answers_val)\n",
    "print('Logistic Regression accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3abd7dp-guM",
    "outputId": "ca16e6f0-e99d-4723-80e0-e1df2e384b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy with scaling: 0.6223241590214067\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression on scaled data\n",
    "clf = LogisticRegression()\n",
    "clf.fit(scaled_train_df_vec, answers_train)\n",
    "answers_val_pred = clf.predict(scaled_val_df_vec)\n",
    "score = accuracy_score(answers_val_pred, answers_val)\n",
    "print('Logistic Regression accuracy with scaling:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psPMYdO-8Eed"
   },
   "source": [
    "SVM and Linear Regression have slightly better accuracy then the baseline (but not sufficiently different in most cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwQ9aD3l_z6R"
   },
   "source": [
    "* 2.3. Data vectorizing with BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LlWNfO-rxoS-",
    "outputId": "3d33ba80-cad7-406d-e0c6-92fc3d4cc068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 5.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 34.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Collecting huggingface-hub>=0.0.12\n",
      "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 7.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 58.1 MB/s \n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 31.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l66SIkDWzr-b",
    "outputId": "43c09eab-ca44-4343-ef1b-7236ea51451b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "GdkoY31y5sxm"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(questions, passages, model, tokenizer, max_length):\n",
    "    embeddings = []\n",
    "    for question, passage in zip(questions, passages):\n",
    "        encoded_data = tokenizer.encode_plus(question, passage, max_length=max_length, pad_to_max_length=True, truncation_strategy=\"longest_first\", return_tensors='pt')\n",
    "        input_ids = encoded_data[\"input_ids\"]\n",
    "        attention_mask = encoded_data[\"attention_mask\"]\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "        sentence_embedding = last_hidden_states[0]\n",
    "        embeddings.append(sentence_embedding[:, 0, :].detach().cpu().numpy())\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRt4JudR9f1e",
    "outputId": "218c70e4-8fec-4100-c15a-af17bf9ebd9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "max_length = 256 #I use 256 for fare results with the 3rd chapter. With bigger max_length my google colab is out of memory\n",
    "train_bert_embeddings = get_sentence_embedding(questions_train, passages_train, model=model, tokenizer=tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZviNu7EfDBgZ",
    "outputId": "cf3d8579-1763-42af-e5c1-08ca96d5b9ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "val_bert_embeddings = get_sentence_embedding(questions_val, passages_val, model=model, tokenizer=tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rE4kwtCwe7R"
   },
   "source": [
    "* 2.4. Similarly to 2.1, train SVM, Linear Regression or any other classification model using BERT embeddings as features for a classifier. Describe and analyze your results (use accuracy metric and data from val.jsonl for evaluation of your results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujMMMox9H_G3",
    "outputId": "79e9e538-81ae-4ccc-81af-919e83ee4faa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bert_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "7p6BavuO4pAG"
   },
   "outputs": [],
   "source": [
    "train_bert_embeddings_flattened = [x.flatten() for x in train_bert_embeddings]\n",
    "val_bert_embeddings_flattened = [x.flatten() for x in val_bert_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vMfcpIL6tvB",
    "outputId": "0984e1ca-ff7e-4268-c9a4-7123b5ce5b2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bert_embeddings_flattened[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HM90KDc66LD",
    "outputId": "cbdd0486-4e3b-470e-cb3c-839b4d223b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.6293577981651376\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "clf = SVC()\n",
    "clf.fit(train_bert_embeddings_flattened, answers_train)\n",
    "answers_val_pred = clf.predict(val_bert_embeddings_flattened)\n",
    "score = accuracy_score(answers_val_pred, answers_val)\n",
    "print('SVM accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbQVQzx-BMjC",
    "outputId": "cdd96f85-0ded-40c8-d9ff-0a7475ddc384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 0.6449541284403669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_bert_embeddings_flattened, answers_train)\n",
    "answers_val_pred = clf.predict(val_bert_embeddings_flattened)\n",
    "score = accuracy_score(answers_val_pred, answers_val)\n",
    "print('Logistic Regression accuracy:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93jefq9oLM-v"
   },
   "source": [
    "SVM and Linear Regression with BERT embeddings also have slightly better accuracy then the majority-class baseline and it is almost the same as with word2vec embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyxNZ4dLo-h"
   },
   "source": [
    "### 3. Fine-tune BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8tzE7P6MAO8"
   },
   "source": [
    "* 3.1. Split the data from `train.jsonl` into train and dev (dev_size = 10%) . Tokenize and format the data (do not forget about the [SEP] token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVldDrWsNtuj",
    "outputId": "127ca1e3-ed84-4072-e29b-c8c88b4beb44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (971 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  971\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for question, passage in zip(questions_train, passages_train):\n",
    "    input_ids = tokenizer.encode(question, passage)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAXnPPNJjGtD"
   },
   "source": [
    "The max sentence length is 971. It is longer than 512 (specified max sequence length for BERT). But I can't use 512 due to the google colab out of memory error. So I will use 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z8dy7fm3kDk0"
   },
   "outputs": [],
   "source": [
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMm3Tml8M-ka",
    "outputId": "834bb594-a12d-42db-e573-0dfc37607f89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for question, passage in zip(questions_train, passages_train):\n",
    "    encoded_data = tokenizer.encode_plus(question, passage,\n",
    "                                         add_special_tokens = True,\n",
    "                                         max_length=max_length,\n",
    "                                         pad_to_max_length=True, \n",
    "                                         truncation_strategy=\"longest_first\",\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_tensors='pt')\n",
    "       \n",
    "    input_ids.append(encoded_data['input_ids'])\n",
    "    attention_masks.append(encoded_data['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_train = torch.cat(input_ids, dim=0)\n",
    "attention_masks_train = torch.cat(attention_masks, dim=0)\n",
    "labels_train = torch.tensor(answers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X6oxENf-MI4b"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FCnHQiDbRN-D"
   },
   "outputs": [],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids_train, attention_masks_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aBwI7pSPRXih"
   },
   "outputs": [],
   "source": [
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pZT5rFpRpgP",
    "outputId": "355a039c-1e30-49d3-d0a1-a88dd973d337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,484 training samples\n",
      "  943 validation samples\n"
     ]
    }
   ],
   "source": [
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tTMggQXWLIe"
   },
   "source": [
    "* 3.2. Initialize the model, optimizer and learning rate scheduler. Explain your choice of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "btvZqUiNWj91"
   },
   "outputs": [],
   "source": [
    "#For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 24\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              sampler = RandomSampler(train_dataset),\n",
    "                              batch_size = batch_size)\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(val_dataset,\n",
    "                                   sampler = SequentialSampler(val_dataset),\n",
    "                                   batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TB0x6x9jWJnq",
    "outputId": "879b8d31-26d4-45fb-951a-63b1a7492212"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# The answers on the questions with given passages are yes or no. \n",
    "# So, we have 2 classes and we need to classify given embeddings.\n",
    "# So, I will use BertForSequenceClassification.\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAupJZtJtnpi"
   },
   "source": [
    "I will use Adam optimizer (standard for BERT fine-tuning) with small learning rate: 1e-5. It is better than default 5e-5 and other recommended be BERT authors 3e-5, 2e-5. Also BERT authors recommend to choose number of trainings epochs between 2 and 4, but I'll use 5 epochs. Small learning rate and longer training give better result.\n",
    "In the paper https://arxiv.org/pdf/1905.10044.pdf we have similar approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-m5bAeCVZfOE"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KD8YiHJNZ1MJ"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 5\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgqwES2yLggz"
   },
   "source": [
    "* 3.3. Training loop for BERT fine-tuning (don't forget about evaluation on dev set created in 3.1). \n",
    "\n",
    "**Note:** I follow recommended tutorial for fine-tuning BERT: https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hRMwck0lab2y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BihNixdwbU8s",
    "outputId": "85703c18-3685-4225-e25a-4f686c319231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    354.    Elapsed: 0:01:24.\n",
      "  Batch    80  of    354.    Elapsed: 0:02:47.\n",
      "  Batch   120  of    354.    Elapsed: 0:04:10.\n",
      "  Batch   160  of    354.    Elapsed: 0:05:34.\n",
      "  Batch   200  of    354.    Elapsed: 0:06:57.\n",
      "  Batch   240  of    354.    Elapsed: 0:08:20.\n",
      "  Batch   280  of    354.    Elapsed: 0:09:43.\n",
      "  Batch   320  of    354.    Elapsed: 0:11:06.\n",
      "\n",
      "  Average training loss: 0.65\n",
      "  Training epcoh took: 0:12:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.67\n",
      "  Validation Loss: 0.63\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    354.    Elapsed: 0:01:23.\n",
      "  Batch    80  of    354.    Elapsed: 0:02:46.\n",
      "  Batch   120  of    354.    Elapsed: 0:04:10.\n",
      "  Batch   160  of    354.    Elapsed: 0:05:33.\n",
      "  Batch   200  of    354.    Elapsed: 0:06:56.\n",
      "  Batch   240  of    354.    Elapsed: 0:08:20.\n",
      "  Batch   280  of    354.    Elapsed: 0:09:43.\n",
      "  Batch   320  of    354.    Elapsed: 0:11:07.\n",
      "\n",
      "  Average training loss: 0.59\n",
      "  Training epcoh took: 0:12:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    354.    Elapsed: 0:01:23.\n",
      "  Batch    80  of    354.    Elapsed: 0:02:47.\n",
      "  Batch   120  of    354.    Elapsed: 0:04:10.\n",
      "  Batch   160  of    354.    Elapsed: 0:05:33.\n",
      "  Batch   200  of    354.    Elapsed: 0:06:56.\n",
      "  Batch   240  of    354.    Elapsed: 0:08:20.\n",
      "  Batch   280  of    354.    Elapsed: 0:09:43.\n",
      "  Batch   320  of    354.    Elapsed: 0:11:06.\n",
      "\n",
      "  Average training loss: 0.52\n",
      "  Training epcoh took: 0:12:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.71\n",
      "  Validation Loss: 0.59\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    354.    Elapsed: 0:01:23.\n",
      "  Batch    80  of    354.    Elapsed: 0:02:47.\n",
      "  Batch   120  of    354.    Elapsed: 0:04:10.\n",
      "  Batch   160  of    354.    Elapsed: 0:05:33.\n",
      "  Batch   200  of    354.    Elapsed: 0:06:57.\n",
      "  Batch   240  of    354.    Elapsed: 0:08:20.\n",
      "  Batch   280  of    354.    Elapsed: 0:09:43.\n",
      "  Batch   320  of    354.    Elapsed: 0:11:06.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epcoh took: 0:12:16\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.72\n",
      "  Validation Loss: 0.62\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    354.    Elapsed: 0:01:23.\n",
      "  Batch    80  of    354.    Elapsed: 0:02:47.\n",
      "  Batch   120  of    354.    Elapsed: 0:04:10.\n",
      "  Batch   160  of    354.    Elapsed: 0:05:33.\n",
      "  Batch   200  of    354.    Elapsed: 0:06:57.\n",
      "  Batch   240  of    354.    Elapsed: 0:08:20.\n",
      "  Batch   280  of    354.    Elapsed: 0:09:43.\n",
      "  Batch   320  of    354.    Elapsed: 0:11:07.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epcoh took: 0:12:17\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.71\n",
      "  Validation Loss: 0.66\n",
      "  Validation took: 0:00:30\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:03:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#training and validation loss, validation accuracy, timings\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if i % 40 == 0 and not i == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(i, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels,\n",
    "                             )\n",
    "        loss, logits = output[0], output[1]\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            loss, logits = output[0], output[1]\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMaT0mDPvzP_"
   },
   "source": [
    "* 3.4. Make predictions for the test data (from `val.jsonl`),  and analyze the results (use accuracy metric for scoring)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdRt445-bVJh",
    "outputId": "9969cf4c-1746-42ca-a725-dfce0f2425de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for question, passage in zip(questions_val, passages_val):\n",
    "    encoded_data = tokenizer.encode_plus(question, passage,\n",
    "                                         add_special_tokens = True,\n",
    "                                         max_length=max_length,\n",
    "                                         pad_to_max_length=True, \n",
    "                                         truncation_strategy=\"longest_first\",\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_tensors='pt')\n",
    "       \n",
    "    input_ids.append(encoded_data['input_ids'])\n",
    "    attention_masks.append(encoded_data['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_val = torch.cat(input_ids, dim=0)\n",
    "attention_masks_val = torch.cat(attention_masks, dim=0)\n",
    "labels_val = torch.tensor(answers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "JLFDWa_65TyA"
   },
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             sampler = SequentialSampler(test_dataset),\n",
    "                             batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bLu5FJg5nIn",
    "outputId": "f7324079-348c-47d6-aefa-62087300715c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 3,270 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "G26se2oP4cHk"
   },
   "outputs": [],
   "source": [
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZW_ZLTT4gMW",
    "outputId": "4b9b8c53-8cb2-4885-e0e4-d7a5174eaabc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030581039755351\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(flat_predictions, flat_true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7SGLPgAxHiw"
   },
   "source": [
    "We get a good accuracy - 70.3% - after simple BERT fine-tuning. It's a good result comparing to majority-class baseline and base BERT.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDAmoeybxkpn"
   },
   "source": [
    "### 4. Summary & results analysis\n",
    "* 4.1. Compare the results of all the tested models and try to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g-A0yTdz57W"
   },
   "source": [
    "The majority-class baseline accuracy for validation data is 62.2%.\n",
    "\n",
    "Classical linear classifiers give us slightly better results:\n",
    "* 64.1% on SVM with word2vec embeddings;\n",
    "* 63.2% on Logistic Regression with word2vec embeddings;\n",
    "* 66.1% on SVM with scaled word2vec embeddings;\n",
    "* 62.2% on Logistic Regression with scaled word2vec embeddings;\n",
    "* 62.9% on SVM with BERT embeddings;\n",
    "* 64.5% on Logistic Regression with BERT embeddings.\n",
    "\n",
    "I think that there was a correlation between the number of times question words occurred in the passage, but this correlation was not strong enough to build an effective classifier.\n",
    "\n",
    "Fine-tuned BERT neural model give us the best result: 70.3%. Attention mechanism is what we need. Also, I try to use lr=5e-5, epochs=3 and batch_size=16 for fine-tuning and this parameters don't give me similar accuracy (the accuracy was about 63%). So, choosing parameters is a key to get a good accuracy for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'qa_bert_model.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP_QA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
